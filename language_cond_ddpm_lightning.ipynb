{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torchvision\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import PIL\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_task_dictionary = {\n",
    "    0: [\n",
    "        \"Draw the digit zero\",\n",
    "        \"Sketch the number 0\",\n",
    "        \"Inscribe the numeral zero\",\n",
    "        \"Draft the digit 0\",\n",
    "        \"Illustrate the number zero\"\n",
    "    ],\n",
    "    1: [\n",
    "        \"Draw the digit one\",\n",
    "        \"Sketch the number 1\",\n",
    "        \"Inscribe the numeral one\",\n",
    "        \"Draft the digit 1\",\n",
    "        \"Illustrate the number one\"\n",
    "    ],\n",
    "    2: [\n",
    "        \"Draw the digit two\",\n",
    "        \"Sketch the number 2\",\n",
    "        \"Inscribe the numeral two\",\n",
    "        \"Draft the digit 2\",\n",
    "        \"Illustrate the number two\"\n",
    "    ],\n",
    "    3: [\n",
    "        \"Draw the digit three\",\n",
    "        \"Sketch the number 3\",\n",
    "        \"Inscribe the numeral three\",\n",
    "        \"Draft the digit 3\",\n",
    "        \"Illustrate the number three\"\n",
    "    ],\n",
    "    4: [\n",
    "        \"Draw the digit four\",\n",
    "        \"Sketch the number 4\",\n",
    "        \"Inscribe the numeral four\",\n",
    "        \"Draft the digit 4\",\n",
    "        \"Illustrate the number four\"\n",
    "    ],\n",
    "    5: [\n",
    "        \"Draw the digit five\",\n",
    "        \"Sketch the number 5\",\n",
    "        \"Inscribe the numeral five\",\n",
    "        \"Draft the digit 5\",\n",
    "        \"Illustrate the number five\"\n",
    "    ],\n",
    "    6: [\n",
    "        \"Draw the digit six\",\n",
    "        \"Sketch the number 6\",\n",
    "        \"Inscribe the numeral six\",\n",
    "        \"Draft the digit 6\",\n",
    "        \"Illustrate the number six\"\n",
    "    ],\n",
    "    7: [\n",
    "        \"Draw the digit seven\",\n",
    "        \"Sketch the number 7\",\n",
    "        \"Inscribe the numeral seven\",\n",
    "        \"Draft the digit 7\",\n",
    "        \"Illustrate the number seven\"\n",
    "    ],\n",
    "    8: [\n",
    "        \"Draw the digit eight\",\n",
    "        \"Sketch the number 8\",\n",
    "        \"Inscribe the numeral eight\",\n",
    "        \"Draft the digit 8\",\n",
    "        \"Illustrate the number eight\"\n",
    "    ],\n",
    "    9: [\n",
    "        \"Draw the digit nine\",\n",
    "        \"Sketch the number 9\",\n",
    "        \"Inscribe the numeral nine\",\n",
    "        \"Draft the digit 9\",\n",
    "        \"Illustrate the number nine\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1681409/4156803524.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  task_text_embeddings = torch.tensor([task_text_embeddings[i] for i in range(10)])\n"
     ]
    }
   ],
   "source": [
    "text_encoder =  SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "task_text_embeddings = {}\n",
    "for class_label, formulations in digit_task_dictionary.items():\n",
    "    # Convert the formulations into embeddings\n",
    "    task_text_embeddings[class_label] = text_encoder.encode(formulations)\n",
    "\n",
    "# Convert to tensor of shape (10, 5, 384)\n",
    "task_text_embeddings = torch.tensor([task_text_embeddings[i] for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondDDPM(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Predicts the noise level for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        input (torch.Tensor): A tensor representing a batch of noisy images.\n",
    "        (batch_size, num_channels, height, width)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor representing the predicted noise level for each image \n",
    "            in the batch.(batch_size, num_channels, height, width)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, noise_predictor,\n",
    "            mask_probability=0.5, \n",
    "            num_diff_steps=1000,\n",
    "            beta_min = 1e-4,\n",
    "            beta_max = 1e-2,\n",
    "            w = 0.3, \n",
    "            task_text_embeddings = task_text_embeddings\n",
    "    ) -> None: \n",
    "        super().__init__()\n",
    "        self.noise_predictor = noise_predictor\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.mask_probability = mask_probability\n",
    "        self.num_diff_steps = num_diff_steps\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        betas, alphas, alpha_bars = self.get_alpha_bars()\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas\", alphas)\n",
    "        self.register_buffer(\"alpha_bars\", alpha_bars)\n",
    "        self.w = 0.3\n",
    "        self.register_buffer(\"task_text_embeddings\", task_text_embeddings)\n",
    "\n",
    "    def get_alpha_bars(self):\n",
    "        betas = torch.linspace(self.beta_min, self.beta_max, self.num_diff_steps)\n",
    "        alphas = 1 - betas\n",
    "        alpha_bars = torch.cumprod(alphas, dim = 0)\n",
    "        return betas, alphas, alpha_bars\n",
    "    \n",
    "\n",
    "    def prepare_texts(self, y):\n",
    "        \"\"\"Takes in (batch_size, 1), converts to randomly sampled texts,\n",
    "        mask out with self.mask_probability. Outputs (batch_size, num_text_tokens, text_embedding_size)\n",
    "        WIP, naive copilot implementation\n",
    "        \"\"\"\n",
    "\n",
    "        # Choose random task formulation embedding for each sample\n",
    "        y = y.view(-1)\n",
    "        batch_size = y.shape[0]\n",
    "        num_examples = self.task_text_embeddings.shape[1]\n",
    "        random_indices = torch.randint(num_examples, size=(batch_size,))\n",
    "        chosen_embeddings = self.task_text_embeddings[y, random_indices]\n",
    "        \n",
    "        # Mask out the whole embedding (row) with probability self.mask_probability\n",
    "        mask = torch.bernoulli(torch.ones(batch_size) - self.mask_probability).to(chosen_embeddings.device)\n",
    "        mask = mask.view(-1, 1).expand_as(chosen_embeddings)\n",
    "        #print(\"---- devices in prepare_texts ----\")\n",
    "        #print(f\"chosen_embeddings: {chosen_embeddings.device}\")\n",
    "        #print(f\"mask: {mask.device}\")\n",
    "        \n",
    "        chosen_embeddings = chosen_embeddings * mask\n",
    "\n",
    "        return chosen_embeddings\n",
    "    \n",
    "    def forward_diffusion(self, x_0, alpha_bars, noise):\n",
    "        assert x_0.shape[0] == alpha_bars.shape[0], f\"Shape of x_0: {x_0.shape}, shape of alpha_bars: {alpha_bars.shape}\"\n",
    "        alpha_bars = alpha_bars.view(-1, 1, 1, 1)\n",
    "        #print(\"---- devices ----\")\n",
    "        #print(f\"x_0: {x_0.device}\")\n",
    "        #print(f\"alpha_bars: {alpha_bars.device}\")\n",
    "        #print(f\"noise: {noise.device}\")\n",
    "        \n",
    "        x_t = torch.sqrt(alpha_bars) * x_0 + torch.sqrt(1 - alpha_bars) * noise\n",
    "        return x_t\n",
    "\n",
    "    def generate_images(self, task_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task_embedding (354,)\n",
    "        Output:\n",
    "            generated_image (1, 1, 28, 28)\n",
    "        \"\"\"\n",
    "        x = torch.randn(1, 1, 28, 28).to(self.device)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            print(\"cudnn is available\")\n",
    "        else:\n",
    "            print(\"cudnn is not available\")\n",
    "        uncond_embedding = torch.zeros_like(task_embedding)\n",
    "        embedding_concat = torch.cat([uncond_embedding, task_embedding], dim=0)\n",
    "\n",
    "        # Pre-allocation\n",
    "        z = torch.zeros(1, 1, 28, 28).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            for t in tqdm(torch.range(self.num_diff_steps-1, 0, -1, dtype=torch.long, device=self.device)):\n",
    "                if t > 0:\n",
    "                    z.normal_()\n",
    "                else:\n",
    "                    z.zero_()\n",
    "\n",
    "                alpha_t = self.alphas[t]\n",
    "                alpha_bar_t = self.alpha_bars[t]\n",
    "                beta_t = self.betas[t]\n",
    "                t = t.float()\n",
    "\n",
    "                x_concat = x.repeat(2, 1, 1, 1)\n",
    "                t_concat = t.repeat(2)\n",
    "\n",
    "                output = self.noise_predictor(x_concat, embedding_concat, t_concat)\n",
    "                noise_pred_uncond = output[0]\n",
    "                noise_pred = output[1]\n",
    "                noise_pred_weighted = (1+self.w)*noise_pred - self.w*noise_pred_uncond\n",
    "\n",
    "                x.sub_(((1-alpha_t)/torch.sqrt(1 - alpha_bar_t)) * noise_pred_weighted)\n",
    "                x.div_(torch.sqrt(alpha_t))\n",
    "                x.add_(torch.sqrt(beta_t) * z)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, c, t):\n",
    "        return self.noise_predictor(x, c, t)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        x, y = batch\n",
    "        masked_y = self.prepare_texts(y)\n",
    "        eps = torch.randn_like(x)\n",
    "        n = len(x)\n",
    "        # sample ts as torch.long from a uniform distribution\n",
    "        ts = torch.randint(0, self.num_diff_steps, (n,), dtype=torch.long, device=self.device)\n",
    "        alpha_bars_ts = self.alpha_bars[ts]\n",
    "        ts = ts.float()\n",
    "        \n",
    "        preds = self.noise_predictor(self.forward_diffusion(x, alpha_bars_ts, eps), masked_y, ts)\n",
    "        loss = self.criterion(preds, eps)\n",
    "        \n",
    "        # log the loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self, lr=1e-3):\n",
    "        return torch.optim.Adam(self.parameters(), lr=lr)\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        query = \"Draw the digit zero\"\n",
    "        query_embedding = torch.tensor(text_encoder.encode(query)).to(self.device)\n",
    "        x = self.generate_images(query_embedding)\n",
    "        #x = torch.randn(1, 1, 28, 28).to(self.device)\n",
    "        x = x.view(-1, 28, 28)\n",
    "        #grid = torchvision.utils.make_grid(x)\n",
    "        images = [image for image in x]\n",
    "        self.logger.log_image(\"samples\", images, self.current_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        '''\n",
    "        standard ResNet style convolutional block\n",
    "        '''\n",
    "        self.same_channels = in_channels==out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            # this adds on correct residual in case channels have increased\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2 \n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        '''\n",
    "        process and downscale the image feature maps\n",
    "        '''\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        '''\n",
    "        process and upscale the image feature maps\n",
    "        '''\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    \"\"\"Used for embedding context and diffusion time step\"\"\"\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        generic one layer FC NN for embedding things  \n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" print(\"--- devices in EmbedFC ---\")\n",
    "        print(f\"x: {x.device} \\n\")\n",
    "        print(f\"self.model: {self.model[0].weight.device} \\n\") \"\"\"\n",
    "\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ContextUnet(nn.Module):\n",
    "    \"\"\" Based on this architecture: https://github.com/TeaPearce/Conditional_Diffusion_MNIST/blob/main/script.py\"\"\"\n",
    "    def __init__(self, in_channels = 1, n_feat = 256, context_dim=384):\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = context_dim\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(self.n_classes, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(self.n_classes, 1*n_feat)\n",
    "\n",
    "        self.up0 = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 7, 7), # otherwise just have 2*n_feat\n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t):\n",
    "        # x is (noisy) image, c is context label, t is timestep, \n",
    "        # context_mask says which samples to block the context on\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        \n",
    "        # embed context, time step\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "\n",
    "        # could concatenate the context embedding here instead of adaGN\n",
    "        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        # up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings\n",
    "        #print(\"shape of cemb1\", cemb1.shape)\n",
    "        #print(\"shape of up1\", up1.shape)\n",
    "        #print(\"shape of temb1\", temb1.shape)\n",
    "        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2+ temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n",
    "\n",
    "\n",
    "ddpm = CondDDPM(ContextUnet())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, title=\"\"):\n",
    "    images = images.detach()\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    cols = math.ceil(len(images) ** (1 / 2))\n",
    "    rows = math.ceil(len(images) / cols)\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            idx = cols * r + c\n",
    "            ax = fig.add_subplot(rows, cols, idx + 1)\n",
    "            ax.axis('off')\n",
    "            if idx < len(images):\n",
    "                ax.imshow(images[idx][0], cmap=\"gray\")\n",
    "    fig.suptitle(title, fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(lambda x: (x - 0.5) * 2)]\n",
    ")\n",
    "dataset = MNIST(\"./datasets\", download=True, train=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size, shuffle=True, num_workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m wandb_logger \u001b[39m=\u001b[39m WandbLogger(project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlanguage-cond-ddpm-lightning\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mTrainer(max_epochs\u001b[39m=\u001b[39;49mnum_epochs, \n\u001b[1;32m      3\u001b[0m                      logger\u001b[39m=\u001b[39;49mwandb_logger, devices\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m      4\u001b[0m                      accelerator\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m                      profiler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msimple\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m                      learning_rate\u001b[39m=\u001b[39;49mlr)\n\u001b[1;32m      7\u001b[0m trainer\u001b[39m.\u001b[39mfit(ddpm, loader)   \n",
      "File \u001b[0;32m~/miniconda3/envs/calvin_venv/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py:340\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m    339\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'learning_rate'"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"language-cond-ddpm-lightning\")\n",
    "trainer = pl.Trainer(max_epochs=num_epochs, \n",
    "                     logger=wandb_logger, devices=1, \n",
    "                     accelerator='gpu',\n",
    "                     profiler=\"simple\")\n",
    "trainer.fit(ddpm, loader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(ddpm.state_dict(), \"checkpoints/lang_cond_ddpm_neew.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1681409/4275961133.py:98: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  for t in tqdm(torch.range(self.num_diff_steps-1, 0, -1, dtype=torch.long, device=self.device)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cudnn is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:40<00:00, 24.42it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load the model\n",
    "ddpm.load_state_dict(torch.load(\"checkpoints/lang_cond_ddpm_new.ckpt\"))\n",
    "\n",
    "query = \"Draw the number 6\"\n",
    "query_embedding = torch.tensor(text_encoder.encode(query))\n",
    "\n",
    "show_images(ddpm.generate_images(query_embedding), title=query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
